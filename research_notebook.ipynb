{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0360d799",
   "metadata": {},
   "source": [
    "# üî¨ Research Notebook ‚Äî Powered by OpenClaw\n",
    "\n",
    "This notebook uses your **Paw / OpenClaw** AI agent to browse the web, gather data from multiple sources, and compile structured findings into a comprehensive research report.\n",
    "\n",
    "**How it works:**\n",
    "1. Connects to your local OpenClaw gateway (started by Paw)\n",
    "2. Sends a research prompt to the agent with web search & scraping tools\n",
    "3. The agent autonomously searches, reads pages, and cross-references facts\n",
    "4. Results are structured into a pandas DataFrame and exported as a report\n",
    "\n",
    "> **Prerequisite:** Make sure Paw is running and the gateway is active on port `18789`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41b281",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "import subprocess, sys\n",
    "for pkg in [\"websockets\", \"pandas\", \"matplotlib\", \"beautifulsoup4\", \"requests\", \"duckduckgo-search\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "                          stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a54703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "import websockets\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"‚úÖ All imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac3446",
   "metadata": {},
   "source": [
    "## 2. Configure Gateway Connection & Agent Settings\n",
    "\n",
    "Connect to your local OpenClaw gateway. Paw starts this automatically on port `18789`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Gateway settings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "GATEWAY_PORT = int(os.environ.get(\"OPENCLAW_PORT\", \"18789\"))\n",
    "GATEWAY_WS_URL = f\"ws://localhost:{GATEWAY_PORT}\"\n",
    "GATEWAY_HTTP_URL = f\"http://localhost:{GATEWAY_PORT}\"\n",
    "\n",
    "# ‚îÄ‚îÄ Agent settings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MAX_SEARCH_RESULTS = 8        # Max results per search query\n",
    "MAX_PAGE_LENGTH = 4000         # Max chars to extract per page\n",
    "MAX_AGENT_ITERATIONS = 15      # Safety limit on agent tool-call loops\n",
    "REQUEST_TIMEOUT = 10           # Seconds for HTTP requests\n",
    "\n",
    "# ‚îÄ‚îÄ Get gateway token ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    resp = requests.get(f\"{GATEWAY_HTTP_URL}/api/health\", timeout=5)\n",
    "    resp.raise_for_status()\n",
    "    health = resp.json()\n",
    "    print(f\"‚úÖ Gateway is running ‚Äî version {health.get('version', 'unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Cannot reach gateway at {GATEWAY_HTTP_URL}: {e}\")\n",
    "    print(\"   Make sure Paw is running and the gateway is started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648fe12",
   "metadata": {},
   "source": [
    "## 3. Define Web Search Function\n",
    "\n",
    "Uses DuckDuckGo search (no API key required) to find relevant sources for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str, max_results: int = MAX_SEARCH_RESULTS) -> list[dict]:\n",
    "    \"\"\"Search the web using DuckDuckGo and return structured results.\"\"\"\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=max_results))\n",
    "        return [\n",
    "            {\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"url\": r.get(\"href\", r.get(\"link\", \"\")),\n",
    "                \"snippet\": r.get(\"body\", r.get(\"snippet\", \"\")),\n",
    "            }\n",
    "            for r in results\n",
    "            if r.get(\"href\") or r.get(\"link\")\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Quick test\n",
    "test_results = web_search(\"OpenClaw AI agent gateway\", max_results=3)\n",
    "print(f\"‚úÖ Web search working ‚Äî got {len(test_results)} results\")\n",
    "for r in test_results[:3]:\n",
    "    print(f\"   ‚Ä¢ {r['title'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc89f4",
   "metadata": {},
   "source": [
    "## 4. Define Web Page Scraping Function\n",
    "\n",
    "Extracts clean text content from a web page using `requests` + `BeautifulSoup`. Handles timeouts, errors, and strips irrelevant HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7552b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str, max_length: int = MAX_PAGE_LENGTH) -> str:\n",
    "    \"\"\"Fetch a URL and extract the main text content.\"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Research Bot; OpenClaw/Paw)\"}\n",
    "        resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Remove non-content elements\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\",\n",
    "                         \"aside\", \"iframe\", \"noscript\", \"form\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        # Try to find main content area\n",
    "        main = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"body\")\n",
    "        if not main:\n",
    "            return \"(Could not extract content)\"\n",
    "\n",
    "        text = main.get_text(separator=\"\\n\", strip=True)\n",
    "        # Clean up excessive whitespace\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "        text = re.sub(r\" {2,}\", \" \", text)\n",
    "\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + \"\\n\\n[‚Ä¶truncated]\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    except requests.Timeout:\n",
    "        return f\"(Timeout fetching {url})\"\n",
    "    except requests.HTTPError as e:\n",
    "        return f\"(HTTP error {e.response.status_code} for {url})\"\n",
    "    except Exception as e:\n",
    "        return f\"(Error fetching {url}: {e})\"\n",
    "\n",
    "# Quick test\n",
    "test_text = scrape_page(\"https://example.com\")\n",
    "print(f\"‚úÖ Scraper working ‚Äî extracted {len(test_text)} chars from example.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915aa745",
   "metadata": {},
   "source": [
    "## 5. Build the Research Agent Loop\n",
    "\n",
    "This connects to your OpenClaw gateway via WebSocket and runs a research agent that can call `web_search` and `scrape_page` tools. The agent decides which sources to search, which pages to read, and when it has enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Tool registry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TOOLS = {\n",
    "    \"web_search\": web_search,\n",
    "    \"scrape_page\": scrape_page,\n",
    "}\n",
    "\n",
    "# Track all searches and scraped pages for analysis\n",
    "research_log: list[dict] = []\n",
    "\n",
    "def execute_tool(name: str, args: dict) -> str:\n",
    "    \"\"\"Execute a tool call and log the result.\"\"\"\n",
    "    if name == \"web_search\":\n",
    "        query = args.get(\"query\", \"\")\n",
    "        print(f\"  üîç Searching: {query}\")\n",
    "        results = web_search(query)\n",
    "        research_log.append({\"type\": \"search\", \"query\": query, \"results\": results})\n",
    "        return json.dumps(results, indent=2)\n",
    "\n",
    "    elif name == \"scrape_page\":\n",
    "        url = args.get(\"url\", \"\")\n",
    "        print(f\"  üìÑ Reading: {url[:80]}\")\n",
    "        content = scrape_page(url)\n",
    "        research_log.append({\"type\": \"scrape\", \"url\": url, \"length\": len(content)})\n",
    "        return content\n",
    "\n",
    "    return f\"Unknown tool: {name}\"\n",
    "\n",
    "\n",
    "async def run_research_agent(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Connect to OpenClaw gateway and run a research session.\n",
    "    The agent uses chat.send to converse and we feed tool results back.\n",
    "    Returns the agent's final compiled research.\n",
    "    \"\"\"\n",
    "    research_log.clear()\n",
    "    request_id = 0\n",
    "\n",
    "    def next_id():\n",
    "        nonlocal request_id\n",
    "        request_id += 1\n",
    "        return request_id\n",
    "\n",
    "    system_prompt = f\"\"\"You are a thorough research assistant. Your task is to research the following topic and compile comprehensive findings.\n",
    "\n",
    "TOPIC: {topic}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Start by searching for the topic from multiple angles (different keywords, perspectives)\n",
    "2. Read the most relevant pages to gather detailed information\n",
    "3. Cross-reference facts across sources ‚Äî note agreements and contradictions\n",
    "4. Collect at least 5 distinct sources before compiling your report\n",
    "5. When you have enough information, compile a final research report\n",
    "\n",
    "For each finding, note:\n",
    "- The source URL and title\n",
    "- Key facts or data points\n",
    "- How reliable/authoritative the source appears\n",
    "\n",
    "YOUR TOOLS:\n",
    "- web_search(query): Search the web. Returns titles, URLs, and snippets.\n",
    "- scrape_page(url): Read a web page's content. Returns extracted text.\n",
    "\n",
    "When you're done researching, write your final report with these sections:\n",
    "## Executive Summary\n",
    "## Key Findings\n",
    "## Detailed Analysis\n",
    "## Sources & References\n",
    "\n",
    "Format your final report in Markdown.\"\"\"\n",
    "\n",
    "    async with websockets.connect(GATEWAY_WS_URL) as ws:\n",
    "        # Handshake ‚Äî connect to gateway\n",
    "        connect_frame = json.dumps([1, next_id(), \"connect\", {\n",
    "            \"name\": \"research-notebook\",\n",
    "            \"scopes\": [\"operator.read\", \"operator.write\", \"operator.admin\"],\n",
    "            \"protocol\": 3\n",
    "        }])\n",
    "        await ws.send(connect_frame)\n",
    "        connect_resp = json.loads(await ws.recv())\n",
    "        if not connect_resp[1]:\n",
    "            raise Exception(f\"Gateway connect failed: {connect_resp}\")\n",
    "        print(\"‚úÖ Connected to OpenClaw gateway\")\n",
    "\n",
    "        # Send the research prompt via chat.send\n",
    "        send_id = next_id()\n",
    "        send_frame = json.dumps([1, send_id, \"chat.send\", {\n",
    "            \"content\": system_prompt,\n",
    "            \"stream\": False\n",
    "        }])\n",
    "        await ws.send(send_frame)\n",
    "        print(\"üì® Research prompt sent ‚Äî agent is working...\\n\")\n",
    "\n",
    "        # Collect the response (may include tool calls)\n",
    "        full_response = \"\"\n",
    "        iterations = 0\n",
    "\n",
    "        while iterations < MAX_AGENT_ITERATIONS:\n",
    "            iterations += 1\n",
    "            try:\n",
    "                raw = await asyncio.wait_for(ws.recv(), timeout=120)\n",
    "            except asyncio.TimeoutError:\n",
    "                print(\"  ‚è∞ Timeout waiting for response\")\n",
    "                break\n",
    "\n",
    "            frame = json.loads(raw)\n",
    "\n",
    "            # Response frame: [1, ok, result, error]\n",
    "            if isinstance(frame, list) and len(frame) >= 3:\n",
    "                # Event frame: [2, event_name, data]\n",
    "                if frame[0] == 2:\n",
    "                    event_name = frame[1]\n",
    "                    event_data = frame[2] if len(frame) > 2 else {}\n",
    "\n",
    "                    if event_name == \"chat.token\":\n",
    "                        token = event_data.get(\"token\", \"\")\n",
    "                        full_response += token\n",
    "\n",
    "                    elif event_name == \"chat.tool_call\":\n",
    "                        tool_name = event_data.get(\"name\", \"\")\n",
    "                        tool_args = event_data.get(\"arguments\", {})\n",
    "                        if isinstance(tool_args, str):\n",
    "                            try:\n",
    "                                tool_args = json.loads(tool_args)\n",
    "                            except json.JSONDecodeError:\n",
    "                                tool_args = {\"query\": tool_args}\n",
    "                        result = execute_tool(tool_name, tool_args)\n",
    "                        # Send tool result back\n",
    "                        inject_id = next_id()\n",
    "                        inject_frame = json.dumps([1, inject_id, \"chat.inject\", {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": result,\n",
    "                            \"name\": tool_name\n",
    "                        }])\n",
    "                        await ws.send(inject_frame)\n",
    "\n",
    "                    elif event_name in (\"chat.done\", \"chat.end\", \"chat.complete\"):\n",
    "                        print(\"\\n‚úÖ Agent finished research\")\n",
    "                        break\n",
    "\n",
    "                # Response to our request\n",
    "                elif frame[0] == 1:\n",
    "                    ok = frame[1]\n",
    "                    result = frame[2] if len(frame) > 2 else None\n",
    "                    if ok and isinstance(result, dict):\n",
    "                        msg = result.get(\"message\", result.get(\"content\", \"\"))\n",
    "                        if msg:\n",
    "                            full_response += msg\n",
    "                    if ok:\n",
    "                        break\n",
    "\n",
    "        print(f\"\\nüìä Research complete ‚Äî {len(research_log)} tool calls, {iterations} iterations\")\n",
    "        return full_response\n",
    "\n",
    "print(\"‚úÖ Research agent ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336ecf2",
   "metadata": {},
   "source": [
    "## 6. Define Research Topic & Execute\n",
    "\n",
    "Change the `RESEARCH_TOPIC` below to whatever you want to research. The agent will autonomously search, read pages, and compile findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë  CHANGE THIS to your research topic                                  ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "RESEARCH_TOPIC = \"Current state of AI agent frameworks in 2026: comparing OpenClaw, LangGraph, CrewAI, and AutoGen\"\n",
    "\n",
    "print(f\"üî¨ Research Topic: {RESEARCH_TOPIC}\")\n",
    "print(f\"   Max iterations: {MAX_AGENT_ITERATIONS}\")\n",
    "print(f\"   Max search results: {MAX_SEARCH_RESULTS}\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "\n",
    "# Run the research agent\n",
    "raw_report = await run_research_agent(RESEARCH_TOPIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273cecb",
   "metadata": {},
   "source": [
    "## 7. Parse and Structure Raw Findings\n",
    "\n",
    "Extract structured data from the research log ‚Äî every search and every page scraped ‚Äî into a clean DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build structured findings from the research log\n",
    "findings = []\n",
    "\n",
    "for entry in research_log:\n",
    "    if entry[\"type\"] == \"search\":\n",
    "        for result in entry.get(\"results\", []):\n",
    "            url = result.get(\"url\", \"\")\n",
    "            domain = urlparse(url).netloc if url else \"unknown\"\n",
    "            findings.append({\n",
    "                \"source\": \"search\",\n",
    "                \"query\": entry[\"query\"],\n",
    "                \"title\": result.get(\"title\", \"\"),\n",
    "                \"url\": url,\n",
    "                \"domain\": domain,\n",
    "                \"snippet\": result.get(\"snippet\", \"\"),\n",
    "                \"content_length\": len(result.get(\"snippet\", \"\")),\n",
    "            })\n",
    "    elif entry[\"type\"] == \"scrape\":\n",
    "        url = entry.get(\"url\", \"\")\n",
    "        domain = urlparse(url).netloc if url else \"unknown\"\n",
    "        findings.append({\n",
    "            \"source\": \"scrape\",\n",
    "            \"query\": \"\",\n",
    "            \"title\": \"\",\n",
    "            \"url\": url,\n",
    "            \"domain\": domain,\n",
    "            \"snippet\": \"\",\n",
    "            \"content_length\": entry.get(\"length\", 0),\n",
    "        })\n",
    "\n",
    "df_findings = pd.DataFrame(findings)\n",
    "\n",
    "if not df_findings.empty:\n",
    "    print(f\"üìä Structured {len(df_findings)} findings from {len(research_log)} tool calls\")\n",
    "    print(f\"   Unique domains: {df_findings['domain'].nunique()}\")\n",
    "    print(f\"   Search queries: {df_findings[df_findings['source'] == 'search']['query'].nunique()}\")\n",
    "    print(f\"   Pages scraped: {len(df_findings[df_findings['source'] == 'scrape'])}\")\n",
    "    display(df_findings[[\"source\", \"domain\", \"title\", \"content_length\"]].head(15))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No findings collected ‚Äî the agent may not have used tools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f50d08",
   "metadata": {},
   "source": [
    "## 8. Deduplicate and Rank Sources\n",
    "\n",
    "Remove duplicate URLs, rank sources by information density, and show a summary table of the best sources found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60543132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_findings.empty:\n",
    "    # Deduplicate by URL\n",
    "    df_unique = df_findings.drop_duplicates(subset=[\"url\"], keep=\"first\").copy()\n",
    "\n",
    "    # Compute a simple relevance score based on content availability\n",
    "    df_unique[\"relevance_score\"] = df_unique.apply(\n",
    "        lambda row: (\n",
    "            (3 if row[\"source\"] == \"scrape\" else 1) +        # Scraped pages are more valuable\n",
    "            min(row[\"content_length\"] / 1000, 5) +            # More content = more relevant\n",
    "            (1 if row[\"snippet\"] else 0)                       # Having a snippet helps\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Sort by relevance\n",
    "    df_ranked = df_unique.sort_values(\"relevance_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(f\"üìã {len(df_ranked)} unique sources (deduplicated from {len(df_findings)})\\n\")\n",
    "    display(df_ranked[[\"domain\", \"title\", \"relevance_score\", \"content_length\"]].head(10))\n",
    "\n",
    "    # ‚îÄ‚îÄ Visualize source distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Domain distribution\n",
    "    domain_counts = df_ranked[\"domain\"].value_counts().head(10)\n",
    "    domain_counts.plot(kind=\"barh\", ax=axes[0], color=\"#6366f1\")\n",
    "    axes[0].set_title(\"Sources by Domain\")\n",
    "    axes[0].set_xlabel(\"Count\")\n",
    "    axes[0].invert_yaxis()\n",
    "\n",
    "    # Source type distribution\n",
    "    type_counts = df_ranked[\"source\"].value_counts()\n",
    "    type_counts.plot(kind=\"pie\", ax=axes[1], autopct=\"%1.0f%%\",\n",
    "                     colors=[\"#6366f1\", \"#22c55e\"], startangle=90)\n",
    "    axes[1].set_title(\"Search Results vs. Scraped Pages\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    df_ranked = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è  No findings to deduplicate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673169d",
   "metadata": {},
   "source": [
    "## 9. Compile Findings into a Formatted Report\n",
    "\n",
    "Display the agent's compiled research report with full Markdown rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9799787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final report\n",
    "report_header = f\"\"\"# Research Report\n",
    "**Topic:** {RESEARCH_TOPIC}\n",
    "**Date:** {datetime.now().strftime(\"%B %d, %Y at %H:%M\")}\n",
    "**Sources consulted:** {len(df_ranked) if not df_ranked.empty else 0}\n",
    "**Tool calls:** {len(research_log)} (searches + page reads)\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Use the agent's compiled report if available, otherwise summarize from findings\n",
    "if raw_report and len(raw_report.strip()) > 100:\n",
    "    final_report = report_header + raw_report\n",
    "else:\n",
    "    # Fallback: build a basic report from the findings data\n",
    "    sources_section = \"\\n\".join(\n",
    "        f\"- [{row.get('title', row['url'])}]({row['url']}) ‚Äî {row['domain']}\"\n",
    "        for _, row in (df_ranked.head(15).iterrows() if not df_ranked.empty else [])\n",
    "    )\n",
    "    final_report = report_header + f\"\"\"## Summary\n",
    "\n",
    "Research was conducted on the topic above. The agent gathered information from {len(df_ranked) if not df_ranked.empty else 0} unique sources.\n",
    "\n",
    "## Sources Found\n",
    "\n",
    "{sources_section if sources_section else \"(No sources collected)\"}\n",
    "\n",
    "> **Note:** The agent's full analysis was not captured in streaming mode.\n",
    "> Re-run with the gateway connected for the full compiled report.\n",
    "\"\"\"\n",
    "\n",
    "# Render the report\n",
    "display(Markdown(final_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fd12c",
   "metadata": {},
   "source": [
    "## 10. Export Report & Data\n",
    "\n",
    "Save the research report as a Markdown file and the raw findings as CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(\"research_output\", exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "topic_slug = re.sub(r\"[^a-z0-9]+\", \"_\", RESEARCH_TOPIC.lower())[:50].strip(\"_\")\n",
    "\n",
    "# Export report as Markdown\n",
    "report_path = f\"research_output/{timestamp}_{topic_slug}.md\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(final_report)\n",
    "print(f\"üìù Report saved: {report_path}\")\n",
    "\n",
    "# Export findings as CSV\n",
    "if not df_ranked.empty:\n",
    "    csv_path = f\"research_output/{timestamp}_{topic_slug}_sources.csv\"\n",
    "    df_ranked.to_csv(csv_path, index=False)\n",
    "    print(f\"üìä Source data saved: {csv_path}\")\n",
    "    print(f\"   {len(df_ranked)} sources, {df_ranked.columns.tolist()}\")\n",
    "\n",
    "# Export raw research log as JSON\n",
    "log_path = f\"research_output/{timestamp}_{topic_slug}_log.json\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"topic\": RESEARCH_TOPIC,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"research_log\": research_log,\n",
    "        \"agent_response_length\": len(raw_report) if raw_report else 0,\n",
    "    }, f, indent=2, default=str)\n",
    "print(f\"üìã Research log saved: {log_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All outputs saved to research_output/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
